### **Big-Data-forecast ‚ú®**
---

Este projeto tem como objetivo realizar previs√µes de vendas semanais, utilizando t√©cnicas de Big Data, com foco em agrega√ß√£o de dados, engenharia de features e modelagem preditiva.

---

### **Estrutura do Projeto üìÇ**

* **`src/`**: Cont√©m o c√≥digo principal do nosso pipeline, dividido em scripts para cada etapa do processo.
* **`notebooks/`**: Notebooks Jupyter para an√°lises explorat√≥rias, testes e visualiza√ß√µes dos resultados.
* **`data/raw/`**: Onde os dados brutos (como o arquivo `.zip`) devem ser armazenados.
* **`data/processed/`**: Armazena os dados intermedi√°rios e a base final com features, prontos para a modelagem.
* **`models/`**: Pasta para salvar o modelo de previs√£o treinado e os arquivos de configura√ß√£o (como a lista de features selecionadas).
* **`requirements.txt`**: Lista todas as bibliotecas necess√°rias para o projeto.
* **`README.md`**: Este arquivo, que serve como guia para o projeto.

---

### **O Pipeline do Projeto üöÄ**

Nosso projeto segue um fluxo de trabalho estruturado para garantir a melhor performance e precis√£o:

1.  **Carregar e Juntar os Dados**: Lemos os arquivos `Parquet` de dentro do arquivo `.zip` e os unimos em uma √∫nica base de dados consolidada.
2.  **Engenharia de Features**: Criamos novas vari√°veis poderosas a partir dos dados brutos, como o ranque de semanas (`week_rank`), a semana do m√™s (`week_of_month`) e features defasadas (`lagged features`) que olham para o passado para prever o futuro.
3.  **Sele√ß√£o de Features**: Utilizamos uma abordagem combinada para encontrar as melhores vari√°veis:
    * **Triagem R√°pida**: Removemos features com pouca import√¢ncia no modelo.
    * **Backward Selection**: Para as features restantes, removemos uma por uma e avaliamos o impacto no WMAPE, garantindo que a remo√ß√£o de uma vari√°vel n√£o prejudique o modelo.
4.  **Treinamento do Modelo Final**: Treinamos o modelo CatBoost com a base de dados completa de 2022, usando apenas as features que selecionamos.
5.  **Previs√£o e Scoragem**: Geramos as previs√µes para janeiro de 2023, semana a semana, usando o modelo treinado.

---

### **O Modelo Escolhido: CatBoost üéØ**

Para este projeto, utilizamos o **CatBoost**, um modelo de `gradient boosting` de √∫ltima gera√ß√£o.

**Vantagens do CatBoost:**
* **Performance e GPU**: O CatBoost √© extremamente r√°pido, especialmente com a acelera√ß√£o de **GPU**, tornando o treinamento mais eficiente.
* **Categorias Nativas**: Ele lida com vari√°veis categ√≥ricas de forma nativa e otimizada, o que nos poupa do pr√©-processamento manual e de problemas como a alta dimensionalidade.
* **Robustez**: Ele √© projetado para evitar `overfitting`, sendo ideal para o nosso projeto.

---

### **Configura√ß√£o do Ambiente ‚òÅÔ∏è**

Este projeto foi desenvolvido e testado no **Google Colab**, pois sua integra√ß√£o com o PySpark e acesso a GPUs √© facilitada.

Para uma execu√ß√£o sem problemas e com o m√°ximo de performance, recomendamos a seguinte configura√ß√£o de hardware:

* **GPU**: Uma **NVIDIA Tesla A100** para acelerar o treinamento do CatBoost.
* **Mem√≥ria RAM**: **64GB ou mais** para lidar com o volume total de dados e evitar erros de `OutOfMemoryError` durante as opera√ß√µes mais pesadas do PySpark.

---

### **Como executar ‚öôÔ∏è**

1.  Abra o seu notebook no Google Colab.
2.  Monte o seu Google Drive para que o notebook possa acessar os dados e salvar os resultados.
3.  Instale as bibliotecas necess√°rias usando o `requirements.txt`.
4.  Execute as c√©lulas do notebook em sequ√™ncia, seguindo a l√≥gica do pipeline.
